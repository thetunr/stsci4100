---
title: "STSCI 4100: IDMb Analysis Project"
author: "Tony Oh (do256), William Rhee (wr86)"
date: "2025-05-16"
output:
  html_document:
    df_print: paged
---


| **TABLE OF CONTENTS**            |
|----------------------------------|
| **1. Introduction**              |
| **2. Data Exploration**          |
| *The basics dataset*             |
| *The crew dataset*               |
| *The principals dataset*         |
| *A side-note*                    |
| *The ratings dataset*            |
| **3. Data Analysis**             |
| *Question 1*                     |
| *Question 2*                     | 
| *Question 3*                     |
| *Question 4*                     |
| **4. Summary**                   |


\newpage

# [1] INTRODUCTION:
IMDb documents reviews of released movies and films in non-commercial datasets — these datasets contain vast amounts of metadata regarding viewer ratings, genres, release years, and more. In this project, we want to understand if these factors dictate any trends in film popularity and genre success.

In this project, we will be using four datasets from IMDb:

1. **title.basics.tsv**: A dataset providing all the basic details about a broad range of films from 1892 to 2026.
2. **title.crew.tsv**: A dataset providing the ID's specifically of the director and writers for the movies from Dataset 1.
3. **title.principals.tsv**: A dataset providing details about other crew (editors, producers, etc.) for the movies from Dataset 1.
4. **title.ratings.tsv**: A dataset that contains the rating data from IMDb's users for the movies from Dataset 1,

Our analysis focuses on a **wide range of films** — from shorts, TV series, movies, and even video content — in the period **1892 to 2026**. We will be answering **four questions** to explore trends in ratings, creative roles, and genres across the whole IMDb dataset.

1. **Ratings**: Does the number of votes per film affect the ratings?
2. **Creative role (writer)**: Does having a writer affect the ratings?
3. **Actor roles**: Does having certain actors have an affect on ratings?
4. **Genres**: Which genres get the highest ratings?

That being said, let's start with data exploration.

---

\newpage

# [2] DATA EXPLORATION:
There are four points in this section:

A.  Importing the Datasets.
B.  A Quick Glance.
C.  Specific Breakdowns.
D.  Combining the Datasets.

## [A] Importing the datasets.

Firstly, to reproduce this project, clone our Git repository found at 
https://github.com/thetunr/stsci4100. Then, open this file (`analysis.Rmd`) and
set your working directory to this project's root directory. 

We provide two methods in importing and saving the datasets.

1. Download the exact RDS files we worked on. This method will be most accurate
with the results we found in our analysis. To access these files, download the 
four RDS files from 
https://drive.google.com/drive/folders/1IQp6n6-yVzPhkcDjmhk52B3E0fm8emWi?usp=sharing. 
Once the download is complete, place the four RDS files into the folder `./rds` 
of the project's root directory. 
2. Download the updated IMDb dataset from the IMDb source. This method may not 
align with the results we found in our analysis. To use this method of 
retrieving the datasets, follow the instructions detailed in `import_save.Rmd`.  

Once you have the RDS files set up in the `./rds` directory, run the code chunk
below to read in the RDS data. 

```{r read-rds-files, eval = FALSE}
library(data.table)
# gc() # Clear unused memory
# SET WORKING DIRECTORY TO PROJECT ROOT IF NOT YET DONE

rds_dir <- "rds/"
basics <- readRDS(paste0(rds_dir, "basics.rds")); 
crew <- readRDS(paste0(rds_dir, "crew.rds")); 
principals <- readRDS(paste0(rds_dir, "principals.rds")); 
ratings <- readRDS(paste0(rds_dir, "ratings.rds")); 

list.of.datasets <- list(basics, crew, principals, ratings)
attr(list.of.datasets, "names") <- c("basics", "crew", "principals", "ratings")
```

**IMPORTANT NOTE**: Note that this set of datasets is a result of subsetting 
the rows of each dataset in `import_save.Rmd` to match the `tconst` values of 
`ratings` since `ratings` is the limiting dataset in number of rows. More 
detail about this choice can be found in `import_save.Rmd`.


## [B] A Quick Glance.
```{r quick-glance, eval = FALSE}
# Let's view our datasets.

list.heads = FALSE # Set to TRUE to view the heads of datasets
if (list.heads) {
  head(basics); head(crew); head(principals); head(ratings)
}

view.datasets = FALSE # Set to TRUE to view datasets
if (view.datasets) {
  View(basics); View(crew); View(principals); View(ratings)
}

# How many columns are in our datasets?
for (i in names(list.of.datasets)){
  column.names <- paste(colnames(list.of.datasets[[i]]), collapse = ", ")
  print(paste("In the", i, "dataset, there are", 
              ncol(list.of.datasets[[i]]), "columns."))
  print(paste("Its columns are:", column.names))
  cat("\n")
}

# How many rows are in our datasets?
for (i in 1:4) {
  print(paste("There are", 
              nrow(list.of.datasets[[i]]), "rows in the", 
              names(list.of.datasets)[i], "dataset."))
}
```

As a quick reference, here are the datasets we will be looking at:

| Dataset     | Observations  | Variables |
|-------------|---------------|-----------|
| `basics`    | 1,568,336     | 9         |
| `crew`      | 1,568,333     | 3         |
| `principals`| 21,693,507    | 6         |
| `ratings`   | 1,568,336     | 3         |


## [C] Specific Breakdowns.
### [C1] The basics dataset
The **basics** dataset:

   + **tconst**: The unique ID for a specific film.
   + **titleType**: The kind of film — it can be a short, movie, tvShort, and 
   more.
   + **primaryTitle**: The final title of the film.
   + **originalTitle**: The title of the film (before it was changed).
   + **isAdult**: The indicator variable — 1 if it's an adult film, 0 otherwise.
   + **startYear**: The film's release year.
   + **endYear**: The film's ending year, IF the film was a TV show (NA 
   otherwise).
   + **runtimeMinutes**: The film's runtime (in minutes).
   + **genres**: The film's list of genres.

Here is a quick exploration of the **basics** dataset's columns:
```{r basics-dataset, eval = FALSE}
# Investigating the column: tconst. 
anyNA(basics$tconst)
length(unique(basics$tconst)) == nrow(basics)

# Investigating the column: titleType.
anyNA(basics$titleType)
unique(basics$titleType)

# Investigating the column: primaryTitle.
anyNA(basics$primaryTitle)
length(unique(basics$primaryTitle))        

# Investigating the column: originalTitle.
anyNA(basics$originalTitle)
length(unique(basics$originalTitle))

# Investigating the column: isAdult.
anyNA(basics$isAdult)
mean(basics$isAdult)

# Investigating the column: startYear.
anyNA(basics$startYear)
min(basics$startYear, na.rm = TRUE)
max(basics$startYear, na.rm = TRUE)

# Investigating the column: endYear.
anyNA(basics$endYear)
min(basics$endYear, na.rm = TRUE)
max(basics$endYear, na.rm = TRUE)

# Investigating the column: runtimeMinutes.
anyNA(basics$runtimeMinutes)
min(basics$runtimeMinutes, na.rm = TRUE)
max(basics$runtimeMinutes, na.rm = TRUE)
mean(basics$runtimeMinutes, na.rm = TRUE)

# Investigating the column: genres.
anyNA(basics$genres)
```

Here are the takeaways from the given code:

+ **tconst**: There are no missing values. There are exactly 1,568,336 ID's 
(i.e. individual films), one for every row.
+ **titleType**: There are no missing values. There are 10 unique kinds of 
films in this dataset, ranging from shorts to video games.
+ **primaryTitle**: There are no missing values. There are 1,158,575 
primaryTitles in the dataset (< 1,568,336). This should be looked into.
+ **originalTitle**: There are no missing values. There are 1,176,721 
originalTitles in the dataset (< 1,568,336 and > 1,158,575). This should be 
looked into.
+ **isAdult**: There are no missing values. Approximately 1.56% of all the 
films in the dataset are adult films. 
+ **startYear**: There are missing values! Ignoring missing values, the 
earliest release date was 1874, while the latest release date was 2025. 
+ **endYear**: There are missing values! Ignoring missing values, the earliest 
end date was 1932, while the latest end date was 2030.
+ **runtimeMinutes**: There are missing values! Ignoring missing values, the 
shortest film is 0 minutes long, while the longest film is 3,692,080 minutes 
(roughly 61,534 hours) long. The average length of films was 58.39153 minutes. 
+ **genres**: There are missing values!

### [C2] The crew dataset
The **crew** dataset:

   + **tconst**: The unique ID for a specific film.
   + **directors**: The film's list of directors.
   + **writers**: The film's list of writers.

Here is a quick exploration of the **crew** dataset's columns:
```{r crew-dataset, eval = FALSE}
# Investigating the column: tconst.
anyNA(crew$tconst)
length(unique(crew$tconst)) == nrow(crew)

# Investigating the column: directors. 
anyNA(crew$directors)

# Investigating the column: writers.
anyNA(crew$writers)
```

Here are the takeaways from exploring :

+ **tconst**: There are no missing values. There are exactly 1,568,333 ID's 
(i.e. individual films), one for every row. Note that this value is minorly 
less than the number of values in `basics` and `ratings`.
+ **directors**: There are some missing values. This means not every movie in 
our data will have a specified director.
+ **writers**: There are some missing values. This means not every movie in our 
data will have a specified writer.

### [C3] The principals dataset
The **principals** dataset:

   + **tconst**: The unique ID for a specific film.
   + **ordering**: The order in credits.
   + **nconst**: The unique ID for a specific person such as an actor, 
   director, and more.
   + **category**: The role of a specific person such as an actor, director, 
   and more.
   + **job**: The job title such as a producer, editor, and more.
   + **characters**: The character that a specific person has played as.

Here is a quick exploration of the **principals** dataset's columns:
```{r principals-dataset, eval = FALSE}
# Investigating the column: tconst. 
anyNA(principals$tconst)
length(unique(principals$tconst)) == nrow(principals)

# Investigating the column: ordering.
anyNA(principals$ordering)
unique(principals$ordering)

# Investigating the column: nconst.
anyNA(principals$nconst)

# Investigating the column: category.
anyNA(principals$category)
unique(principals$category)

# Investigating the column: job.
anyNA(principals$job)
length(unique(principals$job))

# Investigating the column: characters.
anyNA(principals$characters)
length(unique(principals$characters))
```

Here are the takeaways from the given code:

+ **tconst**: There are no missing values. However, the number of unique film 
ID's does not equal the number of rows — this is because upon close inspection, 
we can see that there are duplicated ID's across several rows. This is 
different from the other three datasets, where there is one unique film ID for 
each row.
+ **ordering**: There are no missing values. The ordering from most to least 
important ranges from 1 to 75.
+ **nconst**: There are no missing values.
+ **category**: There are no missing values. There are 13 unique categories in 
the dataset, from director to casting director.
+ **job**: There are missing values — this is because of the duplicate values 
from the first column. There are 30,167 unique values within this column. 
+ **characters**: There are missing values — this is because of the duplicate 
values from the first column. There are 2,507,211 unique values within this 
column.

### A side-note
We know that there are 13 unique categories within the **category** column. We 
also know that the ordering ranges from 1 to 75, where 1 is most important and 
75 is least important. 

So as a SIDE-NOTE, let's make a DataFrame (and plots) that count how many times 
each job **category** appears in a specific **ordering**.

```{r side-note , eval = FALSE}
par(mfrow = c(5, 3))
# my.table <- table(principals$ordering, principals$category) # Huge: 13 columns

my.table <- principals[, .N, by = .(ordering, category)]
my.table <- dcast(my.table, ordering ~ category, value.var = "N", fill = 0)
my.dataframe <- data.frame(category = character(), ordering = character())
```

```{r plot-side-note, echo = FALSE, fig.width = 10, fig.height = 10}
for (i in 2:ncol(my.table)) {
  # Making the DataFrame:
  order <- which(my.table[[i]] == max(my.table[[i]]))
  update <- data.frame(category = colnames(my.table)[i], 
                       ordering = paste(order, collapse = ", "),
                       stringsAsFactors = FALSE)
  my.dataframe <- rbind(my.dataframe, update)
  
  # Making the plots:
  plot(my.table[[i]],
       xlab = "Assigned orderings",
       ylab = "Frequency",
       main = colnames(my.table)[i],
       pch = 16,
       cex = 0.75)
  abline(v = order, col = "blue", lwd = 2)
  text(x = 40,
       y = 0.5 * max(my.table[[i]]),
       labels = paste("x=", order, collapse = ", "),
       col = "blue",
       cex = 1.25)
}

my.dataframe
```

As we can see, the code matches what we'd expect if we eyeball what the code is 
doing to `my.table`.

### [C4] The ratings dataset
The **ratings** dataset:

   + **tconst**: The unique id for a specific film. There are exactly 1,568,336 
   ID's (i.e. individual films), one for every row.
   + **averageRating**: The average rating, which was given to reviewers on a 
   1-10 scale.
   + **numVotes**: The number of reviewers.

Here is a quick exploration of the **ratings** dataset's columns:
```{r ratings-dataset, eval = FALSE}
# Investigating the column: tconst. 
anyNA(ratings$tconst)
length(unique(ratings$tconst)) == nrow(ratings)

# Investigating the column: averageRating.
anyNA(ratings$averageRating)
unique(ratings$averageRating)
min(ratings$averageRating)
max(ratings$averageRating)
mean(ratings$averageRating)

# Investigating the column: numVotes.
anyNA(ratings$numVotes)
min(ratings$numVotes)
length(which(ratings$numVotes == min(ratings$numVotes)))
max(ratings$numVotes)
length(which(ratings$numVotes == max(ratings$numVotes)))
which(ratings$numVotes == max(ratings$numVotes))
```

Here are the takeaways from the given code:

+ **tconst**: There are no missing values. There are exactly 1,568,336 ID's 
(i.e. individual films), one for every row.
+ **averageRating**: There are no missing values. The worst review ever given 
was 1/10, while the best were 10/10. As a fun fact, the average rating across 
movies from the 1890's all the way to present day is roughly 6.95. 
+ **numVotes**: There are no missing values. Of all 1,568,336 films, 37,066 
films have received the lowest number of reviews (only 5 reviews). Of all 
1,568,336 films, only one film (i.e. the film in the 84,878th row) has the 
largest number of reviews (3,042,484 reviews).

## D. Combining the Datasets.
Let's combine the four datasets:

1. The basics dataset.
2. The crew dataset.
2. The principals dataset.
3. The ratings dataset.

```{r combinding-datasets}
# COMBINING DATASETS:
# [1] Combining basics, crew, and ratings into one data table.  
full_dt <- basics[crew, on = "tconst"]
full_dt <- full_dt[ratings, on = "tconst"]
# View(full_dt) # Uncomment to view data table

# [2] Checking the dimensions (rows, col).
dim(basics)      # This is correct — it should be (1,568,336 rows, 9 columns).
dim(crew)        # This is correct — it should be (1,568,333 rows, 3 columns).
dim(ratings)     # This is correct — it should be (1,568,336 rows, 3 columns).
dim(full_dt)  # This is interesting — we expected (1,568,333 rows, 13 columns). 
# This is because crew is a limiting dataset as it has less rows than others. 
```

As we can see:

1. We are able to merge `basics` and `ratings` based on the first column. This 
is because all of their values within the first column match.
2. However, our final dataset contains 1,568,336 rows. It has the right number 
of columns, but it seems that we are **not** being bottlenecked by the number 
of rows in crews. This may be a problem later when we try to do analysis on the 
crew data but some are missing from `full_dt`.

Let's run diagnostics to see why this is happening.
```{r diagnostics-1, eval = FALSE}
# DIAGNOSTICS: [PART 1]
# [1] Check matching of basics and ratings
# Check: Are all tconst values in ratings represented in basics? 
# Here, it says TRUE. That is good. 
all(ratings$tconst %in% basics$tconst)
# Check: Do all tconst values in basics have ratings data? 
# Here, it says TRUE. That is good. 
all(basics$tconst %in% ratings$tconst)

# [2] Check matching of basics and crew
# Check: Are all tconst values in crew represented in basics? 
# Here, it says TRUE. That is good. 
all(crew$tconst %in% basics$tconst)
# Check: Do all tconst values in basics have crew data? 
# Here, it says FALSE. That may be an issue. 
all(basics$tconst %in% crew$tconst)

# [3] Check matching of basics and crew
# Check: Are all tconst values in principals represented in basics? 
# Here, it says TRUE. That is good. 
all(principals$tconst %in% basics$tconst)
# Check: Do all tconst values in basics have principals data? 
# Here, it says FALSE. That may be an issue. 
all(basics$tconst %in% principals$tconst)
```

Here, we found out that both `crew` and `principals` lack `tconst` values 
compared to `basics`. That means we may have to trim all datasets to match that 
of `crew` when doing analysis that requires all datasets. 

However, we have checked that `basics` and `ratings` match in its `tconst` 
values. This is due to the fact that we initially subsetted all datasets based 
on `ratings`. `basics` had `tconst` values for every film and was simply 
subsetted by `ratings`.

Since we have found a mismatch between `crew`, `principals` and the two other 
datasets, we will subset those datasets once again to provide consistency 
throughout our data. 

```{r diagnostics-2, eval = FALSE}
# DIAGNOSTICS: [PART 2]
# [1] Finding bottlenecking dataset
length(unique(basics$tconst))     # This is expected. 
length(unique(crew$tconst))       # This could be a bottleneck. 
length(unique(principals$tconst)) # This seems like the real bottleneck.  
length(unique(ratings$tconst))    # This is expected. 

# [2] Using principals bottleneck on other datasets
principals_tconsts <- unique(principals$tconst)
basics <- basics[tconst %in% principals_tconsts]
crew <- crew[tconst %in% principals_tconsts]
ratings <- ratings[tconst %in% principals_tconsts]

# [3] Check matching
length(unique(basics$tconst))     # Output: 1542357
length(unique(crew$tconst))       # Output: 1542357
length(unique(principals$tconst)) # Output: 1542357
length(unique(ratings$tconst))    # Output: 1542357
```

Now, all the datasets have the same set of unique `tconst` values.

However, by the nature of the `principals` dataset, we can't store its multiple 
rows for each `tconst` value into one row in our final data table.  

```{r diagnostics-3, eval = FALSE}
# DIAGNOSTICS: [PART 3]
# [1] Check whether the datasets can be merged
length(basics$tconst)     # This is expected. 
length(crew$tconst)       # This is expected. 
length(principals$tconst) # This needs to be fixed.
length(ratings$tconst)    # This is expected. 

# [2] Check one row of principals
principals_first_tconst <- principals[1, tconst]
principals[principals$tconst == principals_first_tconst]
# For this film, it seems that there are four rows associated with it. 

# [3] Modify principals to include data for each tconst in one row
# Use ordering as order of each individual. Index value represents ordering. 
principals <- principals[, .(
  nconst = list(nconst),
  category = list(category),
  job = list(job),
  characters = list(characters)
  ), by = tconst]
head(principals$characters)

# [4] We see that the characters column has an issue with "[\" and "\]". 
# We get rid of those symbols. 
principals[, characters := lapply(characters, function(x) {
  gsub('^\\["|"]$', '', x)
})]
```

Now, we can safely merge all datasets. 

```{r diagnostics-4, eval = FALSE}
# DIAGNOSTICS: [PART 4]
full_dt <- basics[crew, on = "tconst"]
full_dt <- full_dt[principals, on = "tconst"]
full_dt <- full_dt[ratings, on = "tconst"]
nrow(full_dt)
# saveRDS(full_dt, "full_dt.rds") # Run this line to save this cleaned df
full_dt <- readRDS("full_dt.rds")
```

---

\newpage

# [3] DATA ANALYSIS:
Now that we have gone through the datasets and obtained a final data table 
`full_dt` to work with, we'll tackle **five questions** starting with the most 
intuitive one first.

## QUESTION 1:
Our first question is: **does the number of votes affect the ratings?**

In the real world, many people want to know how good a movie is before watching 
it. To do so, they go online to check the movie's reviews. But here's the thing 
everyone suspects:

+ Films that have very few votes (i.e. unpopular films) may not have the most 
reliable ratings, as their reviews don't contain much information.
+ Films that have lots of votes (i.e. popular films) may suggest that the movie 
has larger positive reception or audience appeal.

Specifically, we'll investigate whether or not films that have more votes tend 
to have higher or lower ratings (i.e. whether popularity is correlated with 
perceived quality).

This will provide a good opening introduction, and hopefully provide some 
intuition, to our analysis.
```{r q1-1}
summary(full_dt$numVotes)

par(mfrow = c(1, 2))
# [1] First, let's visualize the distribution of the numVotes column.
hist(full_dt$numVotes,
     xlab = "Number of votes",
     ylab = "Frequency",
     main = "Histogram of Number of Votes",
     col = "azure1")
boxplot(full_dt$numVotes,
        xlab = "Number of votes",
        main = "Boxplot of Number of Votes",
        col = "black",
        pars = list(outpch = 16, outcol = "red"))
# Outliers are colored red, and we see that the box plot deems most of the
# visible points as outliers. The information so far suggests a heavy skew. 
```

```{r, q1-2}
# [2] Second, let's plot the numVotes column (x) against the averageRating column (y).
par(mfrow = c(1, 1))
x <- full_dt$numVotes
y <- full_dt$averageRating
plot(x = x,
     y = y,
     xlab = "Number of votes",
     ylab = "Average rating",
     main = "Number of votes vs. Average rating",
     col = "deepskyblue3")

# So far, the data doesn't seem to follow a strictly linear pattern.
# The data seems to follow some sort of nonlinear pattern.
# So, let's fit a local regression model — a non-parametric statistical method — to visualize the trajectory.
```

```{r, q1-3}
# [3] Lastly, fit a local regression model.
my.lowess <- lowess(log10(x), y)                                                # Fitting a LOWESS model w/default arguments. (Computationally more efficient than LOESS.)
lines(10^my.lowess$x,                                                           # Adds LOWESS curve.
      my.lowess$y,
      col = "deeppink3",
      lwd = 2)

# [4] Check the strength of the linear relationship between these variables.
cor(log10(x), y)
```

The local regression model seems to give us no information on the trend of 
`averageRating` for each film. Let us try plotting on logarithm scale with 
respect to the x-axis. 

To do this, we bin the logarithm of the x-axis and sample a number of values. 

```{r}
# New copy of dt to avoid permanent change to data table
dt <- full_dt
dt$x_log <- log10(full_dt$numVotes)

# Bin by log10 scale
bin_width <- 0.1
dt[, bin := floor(x_log / bin_width) * bin_width]
# Sample 500 (max) per bin
dt_sampled <- dt[, {
  idx <- sample(.N, min(.N, 500))
  .SD[idx]
}, by = bin]
# IQR ribbon
plot_stats <- dt[, .(
  p25 = quantile(averageRating, 0.25),
  p75 = quantile(averageRating, 0.75),
  mean_rating = mean(averageRating),
  votes_bin = 10^mean(x_log)
), by = bin][order(votes_bin)]

# Plot
plot(NA,
     xlim = range(plot_stats$votes_bin),
     ylim = range(dt_sampled$averageRating, na.rm = TRUE),
     log = "x",
     xlab = "Number of votes",
     ylab = "Average rating",
     main = "Average Rating vs. Number of Votes (with spread)")
# 25–75% IQR Shading
polygon(c(plot_stats$votes_bin, rev(plot_stats$votes_bin)),
        c(plot_stats$p25, rev(plot_stats$p75)),
        col = adjustcolor("lightblue", alpha.f = 0.4), border = NA)
# Mean Line
lines(plot_stats$votes_bin, plot_stats$mean_rating, col = "deeppink3", lwd = 2)
# Sampled points
points(dt_sampled$numVotes, dt_sampled$averageRating, 
       pch = 16, col = rgb(0, 0, 0, 10, maxColorValue = 255), cex = 0.5)
```

```{r}
# GAM
library(mgcv)
# Fit GAM
gam_model <- gam(averageRating ~ s(x_log), data = dt)
# Predict
x_pred <- seq(log10(min(dt$numVotes)), log10(max(dt$numVotes)), length.out = 500)
votes_pred <- 10^x_pred
# Predict averageRating
y_pred <- predict(gam_model, newdata = data.frame(x_log = x_pred))

# Plot GAM prediction line
lines(votes_pred, y_pred, col = "forestgreen", lwd = 3, lty = 1)
# Legend
legend("bottomright",
       legend = c("Sampled ratings", "Mean per bin", "25–75% range", "GAM prediction"),
       col = c("black", "deeppink3", rgb(135, 206, 250, maxColorValue = 255), "forestgreen"),
       pch = c(16, NA, 15, NA), lwd = c(NA, 2, NA, 1), lty = c(NA, 1, NA, 1), pt.cex = 1.5)
```

As we can see, the number of votes in our dataset is extremely right-skewed, where:

+ Most movies have very few votes. (Seems to be realistic.)
+ Few movies have lots of votes. (Seems to be realistic; barely in this case.)
+ Since the data is skewed, we performed a log transformation on the number of votes before fitting a local regression curve.

And here is a boxplot in support of our scatterplot with the fitted local regression:
```{r q1-5}
full_dt$votecategories <- NA                                                                                    # Create a new column in full_dt.
full_dt$votecategories[full_dt$numVotes <= 250000] <- "Extremely Low (<250K)"                                # Category 1: Extremely Low
full_dt$votecategories[full_dt$numVotes > 250000 & full_dt$numVotes <= 500000] <- "Low (251K–500K)"       # Category 2: Low
full_dt$votecategories[full_dt$numVotes > 500000 & full_dt$numVotes <= 750000] <- "Medium (501K–750K)"    # Category 3: Medium
full_dt$votecategories[full_dt$numVotes > 750000 & full_dt$numVotes <= 1000000] <- "High (751K-1M)"       # Category 4: High
full_dt$votecategories[full_dt$numVotes > 1000000] <- "Extremely High (>1M)"                                 # Category 5: Extremely High

full_dt$votecategories <- factor(full_dt$votecategories, levels = c("Extremely Low (<250K)",
                                                                          "Low (251K–500K)",
                                                                          "Medium (501K–750K)",
                                                                          "High (751K-1M)",
                                                                          "Extremely High (>1M)"))

boxplot(averageRating ~ votecategories,
        data = full_dt,
        main = "Average Rating by Category",
        xlab = "Vote Count",
        ylab = "Average Rating",
        col = "azure1")
```

Here's what we can take away from this analysis:

1. The plotted graph of average ratings against the number of votes exhibits some nonlinear pattern.
   + Specifically, films that have less votes tend to have a wider spread of average ratings, roughly between 2 to 8.
   + Specifically, films that have more votes tend to have a smaller spread of average ratings.
   + Specifically, the Pearson correlation coefficient is close to zero at around 0.1142457 — this confirms there is no/an extremely weak **linear** relationship, as we visually saw.
2. As the number of votes increases, the average ratings tend to increase with more votes before stabilizing around 7-8.
3. Movies with extremely high votes (at least a million) have narrow/tight distributions with a much higher average rating than the others. This suggests that the number of votes a film receives can act as an indicator of the film's resulting ratings.

\newpage

## QUESTION 2:
The next question we want to answer is: **does having a writer affect the ratings?**

Let's take a step back to understand why we're even asking this question. Upon a quick glance at the dataset, we can see that its column **writers** is filled with a mixture of filled and missing values. In other words, not every film has a specified writer.

As a result, it naturally follows to ask how this would affect the film's ratings. Here's our approach to investigating this:

```{r q2-1}
# [1] Define a binary column called haswriter.
full_dt$haswriter <- as.numeric(!is.na(full_dt$writers))                  #  Binary column: 1 if there's a writer, 0 otherwise.

# [2] Obtain a summary statistic and mean for the column haswriter.
summary(full_dt$haswriter)
mean(full_dt$haswriter)

# Here, the min and max are obviously 0 and 1.
# Here, the mean of the column haswriter is 0.9000014. This means that roughly 90% of our films in the dataset have a specified writer.

# [3] Obtain a summary statistic and mean for the column averageRating.
summary(full_dt$averageRating)
mean(full_dt$averageRating)

# Here, the worst films have an average rating of 1/10. The best films have an average rating of 10/10.
# Here, the average rating is 6.2/10.

# [4] Plot a boxplot of these summary statistics.
boxplot(averageRating ~ haswriter, data = full_dt,
        col = c("red", "blue"),
        names = c("No Writer", "Writer"),
        ylab = "Average Rating",
        main = "Films without vs. with Specified Writers")

# Here, there doesn't seem to be much of a difference.
# Let's conduct a t-test to see if the difference is statistically significant.

# [5] Conduct the t-test.
t.test(averageRating ~ haswriter, data = full_dt)
```

Here's our takeaway from our analysis:

1. In our comparison of films that have specified and unspecified writers, we cam see that their boxplots are identical. Their distribution's span are relatively the same with identical means around 6.
2. There visually doesn't seem to be much of a difference between their means.
3. Just to be safe, we conducted a two-sample t-test. This is where things got interesting:
   + The p-value is 3.045e-16, which is less than an assumed significance level of alpha = 0.05.
   + Technically, this means we reject the null and conclude there is enough evidence to suggest there is a statistically significant difference between these two means.
   + But this obviously does not align with our findings from the dataset.
   + In addition, the t-test's results shows us that the mean for "No writer" is 6.239083 while the mean for "Writer" is 6.146662. Those means are incredibly close and identical.
4. To get around this caveat, we asked ourselves two questions:
   + How could a film not have writers? They do; it's just that some films in the dataset have no specified writers because of data incompleteness, especially for really old or lesser-known films where the writers may have not been well documented.
   + Do all audiences necessarily care about who the writers are for an upcoming film? Unless it's a well-known writer who is mentioned in the trailers, probably not! They likely pay more attention to the director(s) and actors.
   + This is an example where the context of the situation need to be considered, not just a statistical test's dry facts.
   
All in all, having a writer or not in the given dataset does not seem to affect the ratings of a film.

\newpage

## QUESTION 3:
Our third question is: **"Does having certain actors have an affect on ratings? Or is this just an effect of hiring good actors for good movies?"**

To answer this question, we need to realize that we only care about the following columns within the dataset `full_dt`:

+ category: specifically rows that only contain actor information ("actor", "actress"). Unfortunately, not every movie has a specified actor/actress.
+ tconst: The movie that has actor informaiton.
+ averageRating: gives us the average rating of movies with actor information.

Let's first filter out our dataset `full_dt`. We only want the rows where the **category** column contains actor information.
```{r q2-2}
# [1] Create a new dataset called actors.dataset. It's a filtered version of full_dt, only containing the rows that say "actor" and "actress."
actors.dataset <- full_dt[grepl("actor|actress", principals$category), ]
dim(actors.dataset)  # Contains 1,226,885 rows and 17 columns.

# [2] Clean actors.dataset to ONLY have entries that say "actor" and "actresses."
library(data.table)   # Load data.table library — used to handle large datasets.
setDT(actors.dataset) # Convert to data.table.

actors.dataset[, c("nconst", "category", "job", "characters"):={    # Filter each column using mapply(). Because each column was originally a list, return each updated column as a LIST.

  actor.index <- lapply(category, function(cats) cats %in% c("actor", "actress"))
  list(
    mapply(function(x, index) x[index], nconst, actor.index, SIMPLIFY = FALSE),
    mapply(function(x, index) x[index], category, actor.index, SIMPLIFY = FALSE),
    mapply(function(x, index) x[index], job, actor.index, SIMPLIFY = FALSE),
    mapply(function(x, index) x[index], characters, actor.index, SIMPLIFY = FALSE)
  )
}]
```

Here is what we should take away from this dataset:

1. Our filtered dataset `actors.dataset` contains 1,226,885 observations and 17 variables.
2. In other words, out of all given 1,542,357 films, only 1,226,885 of them contain actor information while the rest didn't.
3. A film can have **more than one** actor specified within the **category** column.
4. The dataset does **not always** specify what role the actor played — we can't tell if this is an actor with a big role or not.

That being said, let's conduct our analysis for question 3. Our analysis is broken down into several observations, as listed below.

### Observation 1. Extreme Ratings
The following is our analysis for question 3:
```{r q3-1}
# [1] Setting up our data.
library(data.table)                                                             # Load data.table library — used to handle large datasets.
setDT(actors.dataset)                                                           # Convert to data.table.
long <- actors.dataset[, .(nconst = unlist(nconst),                             # Use unlist() function to unlist each entry — ensures each actor gets own row.
                           averageRating = rep(averageRating, lengths(nconst)),
                           numVotes = rep(numVotes, lengths(nconst)))]     

# [2] Compute each actor's mean rating and number of films.
actor.results <- long[, .(                                                      # A DataFrame w/4 columns.
  weighted.average.rating = sum(averageRating * numVotes) / sum(numVotes),
  total.votes = sum(numVotes),
  number.of.films = .N),
  by = nconst][order(-weighted.average.rating)]                                 # Using order(-average.ratings) orders results from most to least average ratings.

head(actor.results)                                                             # Displays head of DataFrame.
```

Here, we defined a DataFrame that displays a list of actors who starred in films ordered from highest to least average weighted ratings.

As we can see, actors who starred in films with the top ratings (like 10/10) were actors who, **at least according to our dataset**, starred in only a **single film**.

+ This clearly doesn't tell us very much.
+ Why? Because an actor who only starred in only one film that got a 10/10 review will obviously have a weighted average film rating of 10/10. That clearly doesn't say much in our analysis.
+ If an actor who say starred in 1000 films still ended up getting a top weighted rating of 10/10, then that would mean significantly more.

### Caveat 2. Cautionaries
To get our feet even further wet, let's check out some specific actors.
```{r q3-2}
# [1] Bobby Connelly (1909-1922); ID: "nm0175050"
actor.results[which(actor.results$nconst == "nm0175050")]

# [2] Jerold T. Hevener (1873-1947); ID: "nm0381936"
actor.results[which(actor.results$nconst == "nm0381936")]
```

Here, we arbitrarily picked out two actors — Bobby Connelly (1909-1922) and Jerold T. Hevener (1873-1947).

Bobby Connelly was a famous child-actor who unfortunately passed away young. ***According to the dataset**, he starred in 13 films, which seems reasonable given his short career.

However, there seems to be some mistake in the dataset. **According to the dataset**, Jerold T. Hevener was an actor who starred in 10 films. In actuality, he starred in around 36 films.

This quick investigation tells us that we should approach these numbers with caution — the dataset isn't guaranteed to give us back the accurate number of films a specific actor has starred in.

### Observation 3. Converging Ratings
In the previous section, we investigated two arbitrary actors. But now, let's investigate the actor who has starred in the most number of films.
```{r q3-4}
maximum.films <- max(actor.results$number.of.films)
index <- which(actor.results$number.of.films == maximum.films)
actor.results[index]
```

Here, the actor who starred in the most number number of films is **Dee Bradley Baker** (ID: "nm0048389") an extremely well-known voice actor who voices in many films and TV cartoons ever since the early 2000's. According to this dataset, all the 8,230 films he voiced in have an average weighted rating of 7.964585 out of 10 based on 12,753,444 votes. That is a relatively high rating with a large sample voting size.

But here's the catch: does this one actor's situation imply that if you're an actor who starred in **many** films that have **large voting size**, then your films will have an overall higher weighted average rating?

To investigate this, let's do a **Google Search** and **deliberately** investigate other extremely well-known actors who also were in many films.
```{r}
# Famous Actors:
actor.results[which(actor.results$nconst == "nm0000138")]     # [1] Leonardo DiCaprio; ID: "nm0000138"
actor.results[which(actor.results$nconst == "nm0000151")]     # [2] Morgan Freeman; ID: "nm0000151"
actor.results[which(actor.results$nconst == "nm0000158")]     # [3] Tom Hanks; ID: "nm0000158"
actor.results[which(actor.results$nconst == "nm0000375")]     # [4] Robert Downey Jr; ID: "nm0000375"
actor.results[which(actor.results$nconst == "nm0424060")]     # [5] Scarlett Johansson; ID: "nm0424060"

# Average of the Entire Dataset:
mean(actors.dataset$averageRating)
```

As we can see, these chosen famous actors have weighted average ratings between 7.6 and 8, based off of **millions** of votes and **hundreds** of films.

Their weighted average ratings are **slightly higher** than the dataset's global average, which is around 6.928019 out of 10. This suggests that these high-profile actors do tend to appear in **well-received, widely watched** films.

Now here's the thing — as humans, our intuition generally expects high-profile actors to have high ratings such as 9/10 or 8/10. But as we can see, these famous actors' ratings are not necessarily the very highest in the dataset.

In fact, we've already seen actors who had ratings of 10/10. These are specifically actors who were featured in **a smaller number of films** and received **fewer votes**.

This is a classic example of the **Law of Large Numbers**: as the number of films increases, average ratings tend to converge towards the mean.

Colloquially speaking:

+ If you're an actor that was in a just **one** film reviewed by say only **10 people**, and consequently got a perfect 10/10 rating, then your weighted average rating will be a 10/10.
+ If you're an actor that was in a just **one** film reviewed by say only **10 people**, and consequently got a bad 1/10 rating, then your weighted average rating will be a 1/10.
+ Conversely, if you're an actor that was featured in **many films** that were reviewed by **millions of people**, then your resulting weighted average rating will of course likely not be a perfect 10/10 — it's going to be **MUCH LOWER** and **CLOSER** to the entire dataset's mean rating.

Here is a supporting plot that visualizes the Law of Large Numbers:
```{r q3-5}
plot(actor.results$number.of.films,
     actor.results$weighted.average.rating,
     main = "Actor Film Count vs. Average Film Rating",
     xlab = "Number of Films (Per Actor)",
     ylab = "Weighted Average Film Rating",
     col = "palegreen3")
abline(h = mean(actors.dataset$averageRating), col = "deeppink3", lwd = 2)
```

Here, we can see that:

+ Actors who were featured in extremely small amounts of films tended to have extremely high weighted ratings.
+ But as we said earlier, this could potentially be large actors who actually featured in more films than the dataset says, OR small actors whose few films have done well.
+ Actors at the right end of the plot — large actors who acted in many films — had much lower ratings than what's shown on the left side of the plot. Their ratings also tend to be roughly around the average ratings of the entire dataset (roughly around 6).
+ Notice how **similar** this plot looks to the plot from a previous question. This goes back to the **Law of Large Numbers** idea we addressed earlier.

### Observation 4. Large Ratings and Large Voting Size
This part of the analysis does **not** prove that specific actors **cause** their films to have high ratings. Instead, this part of the analysis will hint at how frequently top movies have top actors.

Note: **Just** for this observation, we will (for now) focus on full-length **movies**, excluding TV show episodes.
```{r}
# [1] Find the top 10 movies that have BOTH the highest rating AND highest number of votes.
full_movies <- actors.dataset[titleType == "movie"]                             # Filter to only actual movies
top_movies <- full_movies[order(-averageRating, -numVotes)]                     # Sorting full dataset in order.
top_movies <- top_movies[numVotes > 50000]                                      # I CHOSE at least 50,000 votes to avoid niche films.

top10_movies <- head(top_movies, 10)                                            # Grab the top 10 films.
top10_movies[, .(tconst, nconst, primaryTitle, averageRating, numVotes)]               
# View(top10_movies)                                                            # Uncomment to open the dataset.

# [2] Find the actors that were featured in those top 10 films.
list.of.actors <- list()
for (i in 1:10) {
  list.of.actors[[i]] <- unlist(top10_movies$nconst[i])  # This flattens the inner list
}
names(list.of.actors) <- paste("Movie", 1:10)
# list.of.actors                                                                # Uncomment to see the actors.

# [3] Define a new list to hold actor stats per movie.
actor.stats.by.movie <- list()                                                  # Instantiating a new list to hold actor stats.
for (i in 1:10) {                                                               # Repeat for each movie.
  actors.ids <- list.of.actors[[i]]                                             # Grab the actors within the ith movie.
  stats <- actor.results[nconst %in% actors.ids]                                # Go back to the actor.results dataset. Filter for the specific actors we got.
  actor.stats.by.movie[[i]] <- stats                                            # Update the list.
}
names(actor.stats.by.movie) <- paste("Movie", 1:10)
# actor.stats.by.movie                                                          # Uncomment to see the actors.

# [4] Define a final summarizing table.
movie <- top10_movies$primaryTitle
number.of.actors <- sapply(actor.stats.by.movie, nrow)
avg.weighted.rating <- sapply(actor.stats.by.movie, function(df) mean(df$weighted.average.rating))
avg.number.of.films <- sapply(actor.stats.by.movie, function(df) mean(df$number.of.films))
top.actor.rating <- sapply(actor.stats.by.movie, function(df) max(df$weighted.average.rating))

summarizing.table <- data.frame(
  movie = movie,
  number.of.actors = number.of.actors,
  avg.weighted.rating = avg.weighted.rating,
  avg.number.of.films = avg.number.of.films,
  top.actor.rating = top.actor.rating
)

print(summarizing.table)
```

This is a table that shows the average weighted rating and the average number of films for **all** the actors within **each** movie. It also shows the highest actor rating received among all the actors within **each** movie.

Why do we care about this table? From here, we can compare our table to to the entire dataset's mean, and visually make a plot that shows whether top movies tend to feature top-rated actors.

Here is the plot:
```{r}
plot(movie.summaries.df$avg.weighted.rating,
     type = "b",
     pch = 19,
     col = 4,
     main = "Weighted Average Actor Rating in Top 10 Movies",
     xlab = "Movie Rank (From 1st to 10th Placed Movie)",
     ylab = "Average Actor Weighted Rating (out of 10)",
     ylim = c(5, 10))
abline(h = mean(actors.dataset$averageRating), col = 2, lwd = 2)
legend("bottomleft", legend = c("Film Weighted Average","Entire Dataset Average"), col = c(4, 2), lwd = 2)
```

As we can see, the top 10 **highest-rated**, **popular** movies (excluding episodes) often feature actors with **above-average weighted ratings**. This is indicated by the blue plots consistently placed above the red line.

This plot suggests that great movies do attract talented or well-received actors, but it does NOT guarantee that those actors bring high ratings on their own.

### Conclusion
In conclusion, our analysis shows that while well-known actors tend to star in **well-received** films with **strong** ratings, this does not necessarily mean that their presence **causes** those ratings to be high.

In other words:

+ We do not find clear evidence that certain actors cause higher movie ratings.
+ However, we do find **correlation**: top-rated movies tend to include actors who also have strong average film ratings.

Specifically, we found that:

+ The **extremely high** average ratings, such as a perfect 10/10, mainly come from actors who appeared in only one or two highly-rated, niche films.
+ The weighted average ratings for **major actors** —  Leonardo DiCaprio, Morgan Freeman, etc. — fall between 7.6 and 8.0, which is solidly above average but not necessarily that extremely high.
+ This distinction is a result of the **Law of Large Numbers**, where actors with many films tend to regress toward the dataset’s overall average. On the other hand, actors with just a handful of films with smaller vote counts can only appear to have perfect ratings.
+ Excluding episodes, the **top 10 highest-rated popular movies** often feature actors with above-average weighted ratings.

At the end of the day, the evidence supports the idea that great actors are more likely to have been selected into great films, rather than the idea that actors are **causing** higher ratings. In other words, good movies pick good actors to star (and not always the other way around). 

In actuality, movie quality is likely a result of many factors working in combination with each other **beyond** only the actors, number of votes, and average rating. The director, production value, script, and other factors could be at play.

If we want to look more into the **causes** of highly-rated and highly-popular movies, we'll need to look more into **causal inference**, which is another huge topic worth worth looking into in **future investigations**.

\newpage

## QUESTION 4:
Lastly, we'll finally ask: **which genres get the highest ratings?**

```{r q4-1}
# [1] Define our variables.
unique.length <- length(unique(full_dt$genres))                                  # Defines the length of the unique entries in the column "genres."
#anyNA(full_dt$genres)                                                           # Remember there are missing values in the column "genres."

list.of.genres <- strsplit(unique(full_dt$genres), split = ",")                  # Split all unique genre entries at once
pure.genres <- unique(unlist(list.of.genres))                                    # Unlist and get unique genre labels
pure.genres <- pure.genres[!pure.genres %in% c("Short", NA)]                     # Remove unwanted entries

# Here, list.of.genres is a list of all the genres, each entry being a vector.
# ex: The character "Documentary, Short" will turn into the vector ("Documentary, "Short")

vector.of.genres <- unlist(list.of.genres)                                       # Unpacks a list list.of.genres into a vector; the vector's consequently longer than the list.
pure.genres <- unique(vector.of.genres)                                          # Defines a vector pure.genres that lists all the PURE genres in the entire dataset; much shorter.

# [2] Remove the entries NA and "Short". Those are NOT genres.
pure.genres <- pure.genres[!pure.genres %in% c("Short", NA)]                     # These are all the official genres our dataset uses.
```

For this analysis, we will **assume** that we're working with **pure genres**.

+ We have defined a vector **pure.genres** that contains all 27 unique, pure genres in our dataset.
+ A **pure** genre is the opposite of a combined genre. For instance, our vector's entries say words such as "Action" or "Comedy" rather than something such as "Action Comedy."

It's important to note that our vector **pure.genres** does not include the words "Short" and NA:

+ Long story short: NA is a missing value, while "Short" is not a genre. (It's a format.)
+ In addition, when you quickly look at the column "genres" within our dataset, you will briefly see plenty of rows that contain a genre that says "Short" (and nothing more).
+ This is clearly not helpful, as it tells us nothing about what the genre of the film was.
+ So for our analysis, we made the decision to **ignore** films whose corresponding genre says just "Short" or NA.

```{r q4-2, eval = FALSE}
# Checking how many rows (i.e. films) just say "Short" and NA:
number.of.shorts <- length(which(full_dt[,9] == "Short"))                   # Number of rows in dataset that just say "Short."
number.of.NA <- sum(is.na(full_dt[,9]))                                     # Number of rows in dataset that have missing values.


# Here are the proportions, where the total number of rows is 1542357:
number.of.shorts / 1542357                                                  # Proportion of "Short" rows.
number.of.NA / 1542357                                                      # Proportion of missing value rows.
(number.of.shorts + number.of.NA) / 1542357                                 # Proportion of both "Short" and missing value rows.
```

As we can see, there are 26,904 rows that just say "Short" and 21,464 rows that contain missing values, making up 1.74% and 1.39% of the dataset's total rows. In total, they make up around 3% of the dataset's total rows.

+ That is **really, really small**.
+ So, we'll proceed to ignore those "Short" and missing value rows for our analysis.

```{r q4-3, fig.width = 10, fig.height = 10}
par(mfrow = c(2, 5))

for (i in unique(full_dt$titleType)){                                             # [1] For each kind of film, compute the average ratings for its pure genres.
  number.of.ratings <- c()                                                        # Initialize empty vector — to contain number of ratings found for each pure genre.
  average.ratings <- c()                                                          # Initialize empty vector — to contain weighted average ratings for each pure genre.
  movie.indices <- which(full_dt$titleType == i)
  
  for (j in pure.genres){                                                         # Update both empty vectors.
    indices <- which(grepl(j, full_dt$genres[movie.indices]))
    how.many.ratings <- length(indices)
    
    ratings <- full_dt$averageRating[indices]
    number.of.votes <- full_dt$numVotes[indices]
    weighted.average <- sum(ratings * number.of.votes) / sum(number.of.votes)
    
    average.ratings <- append(average.ratings, weighted.average)
    number.of.ratings <- append(number.of.ratings, how.many.ratings)
  }
  
  ratings.df <- data.frame(genre = pure.genres,                                   # [2] Make a barplot comparing the average ratings for each pure genre.
                           number = number.of.ratings,
                           average = average.ratings)
  ratings.df <- ratings.df[order(ratings.df$average, decreasing = TRUE), ]
  
  my.barplot <- barplot(ratings.df$average, main = paste("Genres for", i),
          names.arg = ratings.df$genre,
          xlab = "Average Rating",
          las = 2,                         
          col = "azure1",
          cex.names = 0.6,
          horiz = TRUE,
          xlim = c(0, 10))
  
  text(x = ratings.df$average,         
       y = my.barplot,                          
       labels = round(ratings.df$average, 2),  
       pos = 4,                         
       cex = 0.7,                       
       col = "black")
}
```

Here is what we should take away from the plot.

1. Each plot displays the least to most ranked genres within each kind of film — shorts, movies, video games, etc.
2. All these plots have genres with weighted average ratings roughly between 5 to 7.
3. This range of average ratings is clearly a lot narrower than we probably intuitively expected to see. Ratings can go from 1-10, but these go from 5-7.
  
Back then, we simply calculated a regular **average** using the mean() function, which incorrectly acts as if all films have received the same number of votes.

+ Consequently, we got (some admittedly) bizarre results.
+ For instance, three of our box plots used to suggest that adult genres have been the most highly rated genres for tvMovies, tvSeries, and tvMiniSeries.

So instead, we computed a **weighted average** rating for each genre.

+ This is crucial because upon looking at the column **numVotes**, we can see that not every film has had the same number of votes. In other words, not every film has had the same number of reviewers.
+ Also, not all box plots have a plotted bar for every genre. For instance, the lower right box plot tells us that there weren't any videogames under the "News" genre (which makes sense). In these kinds of cases, where some genres have zero matched films, the code made sure to automatically exclude those genres from the weighted average calculation.
+ The number of votes clearly influences how each genre's computed average rating will turn out.
+ By computing a weighted average, we are giving less influence to small genres that have skewed/inflated ratings and more influence to larger genres.

Now, what could explain the narrow range of a rating of 5-7?

To start, let's check out the distribution of the averageRating column from our dataset.
```{r q4-4}
par(mfrow = c(1, 1))
hist(full_dt$averageRating,
     col = "azure1",
     xlab = "Average rating",
     ylab = "Frequency",
     main = "Histogram of Full Dataset's Average Rating")
summary(full_dt$averageRating)

ratings <- full_dt$averageRating
number.of.votes <- full_dt$numVotes
weighted.average <- sum(ratings * number.of.votes) / sum(number.of.votes)
abline(v = weighted.average, col = "red", lwd = 4)
```

As we can see:

+ The distribution of the **averageRating** column from our dataset is already indeed narrow, with a center around 7.206151 (i.e. the weighted average of the entire dataset).
+ In addition, the number of ratings used to compute each pure genre's average are all relatively large, with the smallest being only 88 ratings for the genre **Reality-TV**.

There are some possible reasons why the distribution of the full dataset's average rating is clustered between the 5-7 range, even though ratings can go from 1-10.

1. **Selection bias**: People generally review films that they're really interested in or chose to watch voluntarily. They may also review films that they personally find really controversial. Overall, people are likely to give reviews for films that stick out to them for any reason. This could affect the histogram and land its final focus around 5-7.
2. **IMDb anti-scew measures**: IMDb is said to employ anti-scew measures when collecting their survey data — they employ certain weights to ensure that meaningful reviews are counted more while meaningless, jokingly, deliberately, and excessively negative or positive reviews are counted less. This could affect the histogram and land its final focus around 5-7.

Some final notes to conclude this section with:

+ One factor that **could** lead to a change in results is the fact that we had to omit films whose specified genres either said "Short" or "genre." If we knew what the genres of those films actually were, then their contribution could change the outcome of our computed weighted averages.
+ In addition, the average ratings don't necessarily determine what's more popular — it's **also** important to consider other factors such as **total box office revenue**.

---

\newpage

# [4] SUMMARY:

In short, we have learned a lot about our datasets from these four questions. However, these analyses don't cover absolutely everything we can learn from them.

If time permits, we can look further into the following:

1. Comparing movies over the years — movies from the early 19th Century to movies today.
2. Looking to see if there are any more open-source, updated datasets with all actors' films recorded.
3. Considering total box office revenue as part of a larger goal to better understand what factors dictate the success of a movie.
4. Looking into questions for **specific** kinds of film — investigate trends only within movies, or shorts, and so on rather than a whole dataset that deals with a blend of them all.
5. Conducting **causal inference** for the third question.

Overall, this project was a good exercise that blends **technical statistical** knowledge with **real circumstance**. Understanding statistical theory is important, but understanding when and how it should be used is just as, if not more, important.
