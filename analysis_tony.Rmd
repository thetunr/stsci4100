---
title: 'STSCI 4100: IDMb Analysis Project'
author: "Tony Oh (do256), William Rhee (wr86)"
date: "2025-05-16"
output:
  pdf_document: default
  html_document:
    df_print: paged
---


| **TABLE OF CONTENTS**            |
|----------------------------------|
| **1. Introduction**              |
| **2. Data Exploration**          |
| *The basics dataset*             |
| *The crew dataset*               |
| *The principals dataset*         |
| *A side-note*                    |
| *The ratings dataset*            |
| **3. Data Analysis**             |
| *Question 1*                     |
| *Question 2*                     | 
| *Question 3*                     |
| *Question 4*                     |
| **4. Summary**                   |


\newpage

# [1] INTRODUCTION:
IMDb documents reviews of released movies and films in non-commercial datasets — these datasets contain vast amounts of metadata regarding viewer ratings, genres, release years, and more. In this project, we want to understand if these factors dictate any trends in film popularity and genre success.

In this project, we will be using four datasets from IMDb:

1. **title.basics.tsv**: A dataset providing all the basic details about a broad range of films from 1892 to 2026.
2. **title.crew.tsv**: A dataset providing the ID's specifically of the director and writers for the movies from Dataset 1.
3. **title.principals.tsv**: A dataset providing details about other crew (editors, producers, etc.) for the movies from Dataset 1.
4. **title.ratings.tsv**: A dataset that contains the rating data from IMDb's users for the movies from Dataset 1,

Our analysis focuses on a **wide range of films** — from shorts, TV series, movies, and even video content — in the period **1892 to 2026**. We will be answering **four questions** to explore trends in ratings, creative roles, and genres across the whole IMDb dataset.

1. **Ratings**: Does the number of votes per film affect the ratings?
2. **Creative role (writer)**: Does having a writer affect the ratings?
3. **Actor roles**: Does having certain actors have an affect on ratings?
4. **Genres**: Which genres get the highest ratings?

That being said, let's start with data exploration.

---

\newpage

# [2] DATA EXPLORATION:
There are four points in this section:

A.  Importing the Datasets.
B.  A Quick Glance.
C.  Specific Breakdowns.
D.  Combining the Datasets.

## [A] Importing the datasets.

Firstly, to reproduce this project, clone our Git repository found at 
https://github.com/thetunr/stsci4100. Then, open this file (`analysis.Rmd`) and
set your working directory to this project's root directory. 

We provide two methods in importing and saving the datasets.

1. Download the exact RDS files we worked on. This method will be most accurate
with the results we found in our analysis. To access these files, download the 
four RDS files from 
https://drive.google.com/drive/folders/1IQp6n6-yVzPhkcDjmhk52B3E0fm8emWi?usp=sharing. 
Once the download is complete, place the four RDS files into the folder `./rds` 
of the project's root directory. 
  - **IMPORTANT NOTE**: Note that this set of datasets is a result of 
  subsetting the rows of each dataset in `import_save.Rmd` to match the 
  `tconst` values of `ratings` since `ratings` is the limiting dataset in 
  number of rows. That is, we deemed `ratings` to hold information about films 
  that is necessary for all of our analysis. Thus, we removed rows in `basics`, 
  `crew`, and `principals` that represented films that were not represented in 
  `ratings`. A majority of films in `basics`, `crew`, and `principals` that 
  **did not have ratings** were removed according to the films available in 
  `ratings`. More detail about how this choice was executed can be found in 
  `import_save.Rmd`.

2. Download the updated IMDb dataset from the IMDb source. This method may not 
align with the results we found in our analysis. To use this method of 
retrieving the datasets, follow the instructions detailed in `import_save.Rmd`.  

Once you have the RDS files set up in the `./rds` directory, run the code chunk
below to read in the RDS data. 

```{r setup}
library(data.table)
library(mgcv)
gc() # Clear unused memory
# SET WORKING DIRECTORY TO PROJECT ROOT IF NOT YET DONE SO
```

```{r read-rds-files}
rds_dir <- "rds/"
basics <- readRDS(paste0(rds_dir, "basics.rds")); 
crew <- readRDS(paste0(rds_dir, "crew.rds")); 
principals <- readRDS(paste0(rds_dir, "principals.rds")); 
ratings <- readRDS(paste0(rds_dir, "ratings.rds")); 

list.of.datasets <- list(basics, crew, principals, ratings)
attr(list.of.datasets, "names") <- c("basics", "crew", "principals", "ratings")
```


## [B] A Quick Glance.
```{r quick-glance, echo = FALSE}
# Let's view our datasets.

list.heads = FALSE # Set to TRUE to view the heads of datasets
if (list.heads) {
  head(basics); head(crew); head(principals); head(ratings)
}

view.datasets = FALSE # Set to TRUE to view datasets
if (view.datasets) {
  View(basics); View(crew); View(principals); View(ratings)
}

# How many columns are in our datasets?
for (i in names(list.of.datasets)){
  column.names <- paste(colnames(list.of.datasets[[i]]), collapse = ", ")
  print(paste("In the", i, "dataset, there are", 
              ncol(list.of.datasets[[i]]), "columns."))
  print(paste("Its columns are:", column.names))
  cat("\n")
}

# How many rows are in our datasets?
for (i in 1:4) {
  print(paste("There are", 
              nrow(list.of.datasets[[i]]), "rows in the", 
              names(list.of.datasets)[i], "dataset."))
}
```

As a quick reference, here are the datasets we will be looking at:

| Dataset     | Observations  | Variables |
|-------------|---------------|-----------|
| `basics`    | 1,568,336     | 9         |
| `crew`      | 1,568,333     | 3         |
| `principals`| 21,693,507    | 6         |
| `ratings`   | 1,568,336     | 3         |


## [C] Specific Breakdowns.
### [C1] The basics dataset
The **basics** dataset:

   + **tconst**: The unique ID for a specific film.
   + **titleType**: The kind of film — it can be a short, movie, tvShort, and 
   more.
   + **primaryTitle**: The final title of the film.
   + **originalTitle**: The title of the film (before it was changed).
   + **isAdult**: The indicator variable — 1 if it's an adult film, 0 otherwise.
   + **startYear**: The film's release year.
   + **endYear**: The film's ending year, IF the film was a TV show (NA 
   otherwise).
   + **runtimeMinutes**: The film's runtime (in minutes).
   + **genres**: The film's list of genres.

```{r basics-dataset, include = FALSE}
# Here is a quick exploration of the **basics** dataset's columns:

# Investigating the column: tconst. 
anyNA(basics$tconst)
length(unique(basics$tconst)) == nrow(basics)

# Investigating the column: titleType.
anyNA(basics$titleType)
unique(basics$titleType)

# Investigating the column: primaryTitle.
anyNA(basics$primaryTitle)
length(unique(basics$primaryTitle))        

# Investigating the column: originalTitle.
anyNA(basics$originalTitle)
length(unique(basics$originalTitle))

# Investigating the column: isAdult.
anyNA(basics$isAdult)
mean(basics$isAdult)

# Investigating the column: startYear.
anyNA(basics$startYear)
min(basics$startYear, na.rm = TRUE)
max(basics$startYear, na.rm = TRUE)

# Investigating the column: endYear.
anyNA(basics$endYear)
min(basics$endYear, na.rm = TRUE)
max(basics$endYear, na.rm = TRUE)

# Investigating the column: runtimeMinutes.
anyNA(basics$runtimeMinutes)
min(basics$runtimeMinutes, na.rm = TRUE)
max(basics$runtimeMinutes, na.rm = TRUE)
mean(basics$runtimeMinutes, na.rm = TRUE)

# Investigating the column: genres.
anyNA(basics$genres)
```

Here are the takeaways from our analysis of `basis`:

+ **tconst**: There are no missing values. There are exactly 1,568,336 ID's 
(i.e. individual films), one for every row.
+ **titleType**: There are no missing values. There are 10 unique kinds of 
films in this dataset, ranging from shorts to video games.
+ **primaryTitle**: There are no missing values. There are 1,158,575 
primaryTitles in the dataset (< 1,568,336). This should be looked into.
+ **originalTitle**: There are no missing values. There are 1,176,721 
originalTitles in the dataset (< 1,568,336 and > 1,158,575). This should be 
looked into.
+ **isAdult**: There are no missing values. Approximately 1.56% of all the 
films in the dataset are adult films. 
+ **startYear**: There are missing values! Ignoring missing values, the 
earliest release date was 1874, while the latest release date was 2025. 
+ **endYear**: There are missing values! Ignoring missing values, the earliest 
end date was 1932, while the latest end date was 2030.
+ **runtimeMinutes**: There are missing values! Ignoring missing values, the 
shortest film is 0 minutes long, while the longest film is 3,692,080 minutes 
(roughly 61,534 hours) long. The average length of films was 58.39153 minutes. 
+ **genres**: There are missing values!

### [C2] The crew dataset
The **crew** dataset:

   + **tconst**: The unique ID for a specific film.
   + **directors**: The film's list of directors.
   + **writers**: The film's list of writers.

```{r crew-dataset, include = FALSE}
# Investigating the column: tconst.
anyNA(crew$tconst)
length(unique(crew$tconst)) == nrow(crew)

# Investigating the column: directors. 
anyNA(crew$directors)

# Investigating the column: writers.
anyNA(crew$writers)
```

Here are the takeaways from our analysis of `crew`:

+ **tconst**: There are no missing values. There are exactly 1,568,333 ID's 
(i.e. individual films), one for every row. Note that this value is minorly 
less than the number of values in `basics` and `ratings`.
+ **directors**: There are some missing values. This means not every movie in 
our data will have a specified director.
+ **writers**: There are some missing values. This means not every movie in our 
data will have a specified writer.

### [C3] The principals dataset
The **principals** dataset:

   + **tconst**: The unique ID for a specific film.
   + **ordering**: The order in credits.
   + **nconst**: The unique ID for a specific person such as an actor, 
   director, and more.
   + **category**: The role of a specific person such as an actor, director, 
   and more.
   + **job**: The job title such as a producer, editor, and more.
   + **characters**: The character that a specific person has played as.

```{r principals-dataset, include = FALSE}
# Investigating the column: tconst. 
anyNA(principals$tconst)
length(unique(principals$tconst)) == nrow(principals)

# Investigating the column: ordering.
anyNA(principals$ordering)
unique(principals$ordering)

# Investigating the column: nconst.
anyNA(principals$nconst)

# Investigating the column: category.
anyNA(principals$category)
unique(principals$category)

# Investigating the column: job.
anyNA(principals$job)
length(unique(principals$job))

# Investigating the column: characters.
anyNA(principals$characters)
length(unique(principals$characters))
```

Here are the takeaways from our analysis of `principals`:

+ **tconst**: There are no missing values. However, the number of unique film 
ID's does not equal the number of rows — this is because upon close inspection, 
we can see that there are duplicated ID's across several rows. This is 
different from the other three datasets, where there is one unique film ID for 
each row.
+ **ordering**: There are no missing values. The ordering from most to least 
important ranges from 1 to 75.
+ **nconst**: There are no missing values.
+ **category**: There are no missing values. There are 13 unique categories in 
the dataset, from director to casting director.
+ **job**: There are missing values — this is because of the duplicate values 
from the first column. There are 30,167 unique values within this column. 
+ **characters**: There are missing values — this is because of the duplicate 
values from the first column. There are 2,507,211 unique values within this 
column.

### A side-note
We know that there are 13 unique categories within the **category** column. We 
also know that the ordering ranges from 1 to 75, where 1 is most important and 
75 is least important. 

So as a SIDE-NOTE, let's make a DataFrame (and plots) that count how many times 
each job **category** appears in a specific **ordering**.

```{r side-note, echo = FALSE, fig.width = 10, fig.height = 10}
par(mfrow = c(5, 3))
my.table <- principals[, .N, by = .(ordering, category)]
my.table <- dcast(my.table, ordering ~ category, value.var = "N", fill = 0)
my.dataframe <- data.frame(category = character(), ordering = character())

for (i in 2:ncol(my.table)) {
  # Making the DataFrame:
  order <- which(my.table[[i]] == max(my.table[[i]]))
  update <- data.frame(category = colnames(my.table)[i], 
                       ordering = paste(order, collapse = ", "),
                       stringsAsFactors = FALSE)
  my.dataframe <- rbind(my.dataframe, update)
  
  # Making the plots:
  plot(my.table[[i]],
       xlab = "Assigned orderings",
       ylab = "Frequency",
       main = colnames(my.table)[i],
       pch = 16,
       cex = 0.75)
  abline(v = order, col = "blue", lwd = 2)
  text(x = 40,
       y = 0.5 * max(my.table[[i]]),
       labels = paste("x=", order, collapse = ", "),
       col = "blue",
       cex = 1.25)
}

my.dataframe
```

As we can see, the code matches what we'd expect if we eyeball what the code is 
doing to `my.table`.

### [C4] The ratings dataset
The **ratings** dataset:

   + **tconst**: The unique id for a specific film. There are exactly 1,568,336 
   ID's (i.e. individual films), one for every row.
   + **averageRating**: The average rating, which was given to reviewers on a 
   1-10 scale.
   + **numVotes**: The number of reviewers.

```{r ratings-dataset, include = FALSE}
# Investigating the column: tconst. 
anyNA(ratings$tconst)
length(unique(ratings$tconst)) == nrow(ratings)

# Investigating the column: averageRating.
anyNA(ratings$averageRating)
unique(ratings$averageRating)
min(ratings$averageRating)
max(ratings$averageRating)
mean(ratings$averageRating)

# Investigating the column: numVotes.
anyNA(ratings$numVotes)
min(ratings$numVotes)
length(which(ratings$numVotes == min(ratings$numVotes)))
max(ratings$numVotes)
length(which(ratings$numVotes == max(ratings$numVotes)))
which(ratings$numVotes == max(ratings$numVotes))
```

Here are the takeaways from our analysis of `ratings``:

+ **tconst**: There are no missing values. There are exactly 1,568,336 ID's 
(i.e. individual films), one for every row.
+ **averageRating**: There are no missing values. The worst review ever given 
was 1/10, while the best were 10/10. As a fun fact, the average rating across 
movies from the 1890's all the way to present day is roughly 6.95. 
+ **numVotes**: There are no missing values. Of all 1,568,336 films, 37,066 
films have received the lowest number of reviews (only 5 reviews). Of all 
1,568,336 films, only one film (i.e. the film in the 84,878th row) has the 
largest number of reviews (3,042,484 reviews).

## D. Combining the Datasets.
Let's combine the four datasets:

1. The basics dataset.
2. The crew dataset.
2. The principals dataset.
3. The ratings dataset.

```{r combinding-datasets}
# COMBINING DATASETS:
# [1] Combining basics, crew, and ratings into one data table.  
full_dt <- basics[crew, on = "tconst"]
full_dt <- full_dt[ratings, on = "tconst"]
# View(full_dt) # Uncomment to view data table

# [2] Checking the dimensions (rows, col).
dim(basics)      # This is correct — it should be (1,568,336 rows, 9 columns).
dim(crew)        # This is correct — it should be (1,568,333 rows, 3 columns).
dim(ratings)     # This is correct — it should be (1,568,336 rows, 3 columns).
dim(full_dt)  # This is interesting — we expected (1,568,333 rows, 13 columns). 
# This is because crew is a limiting dataset as it has less rows than others. 
```

As we can see:

1. We are able to merge `basics` and `ratings` based on the first column. This 
is because all of their values within the first column match.
2. However, our final dataset contains 1,568,336 rows. It has the right number 
of columns, but it seems that we are **not** being bottlenecked by the number 
of rows in crews. This may be a problem later when we try to do analysis on the 
crew data but some are missing from `full_dt`.

Let's run diagnostics to see why this is happening.
```{r diagnostics-1}
# DIAGNOSTICS: [PART 1]
# [1] Check matching of basics and ratings
# Check: Are all tconst values in ratings represented in basics? 
all(ratings$tconst %in% basics$tconst)
# Here, it says TRUE. That is good. 

# Check: Do all tconst values in basics have ratings data? 
all(basics$tconst %in% ratings$tconst)
# Here, it says TRUE. That is good. 

# [2] Check matching of basics and crew
# Check: Are all tconst values in crew represented in basics? 
all(crew$tconst %in% basics$tconst)
# Here, it says TRUE. That is good. 

# Check: Do all tconst values in basics have crew data? 
all(basics$tconst %in% crew$tconst)
# Here, it says FALSE. That may be an issue. 

# [3] Check matching of basics and crew
# Check: Are all tconst values in principals represented in basics? 
all(principals$tconst %in% basics$tconst)
# Here, it says TRUE. That is good. 

# Check: Do all tconst values in basics have principals data? 
all(basics$tconst %in% principals$tconst)
# Here, it says FALSE. That may be an issue. 
```

Here, we found out that both `crew` and `principals` lack `tconst` values 
compared to `basics`. That means we may have to trim all datasets to match that 
of `crew` when doing analysis that requires all datasets. 

However, we have checked that `basics` and `ratings` match in its `tconst` 
values. This is due to the fact that we initially subsetted all datasets based 
on `ratings`. `basics` had `tconst` values for every film and was simply 
subsetted by `ratings`.

Since we have found a mismatch between `crew`, `principals` and the two other 
datasets, we will subset those datasets once again to provide consistency 
throughout our data. 

```{r diagnostics-2}
# DIAGNOSTICS: [PART 2]
# [1] Finding bottlenecking dataset
length(unique(basics$tconst))     # This is expected. 
length(unique(crew$tconst))       # This could be a bottleneck. 
length(unique(principals$tconst)) # This seems like the real bottleneck.  
length(unique(ratings$tconst))    # This is expected. 

# [2] Using principals bottleneck on other datasets
principals_tconsts <- unique(principals$tconst)
basics <- basics[tconst %in% principals_tconsts]
crew <- crew[tconst %in% principals_tconsts]
ratings <- ratings[tconst %in% principals_tconsts]

# [3] Check matching
length(unique(basics$tconst))     # Output: 1542357
length(unique(crew$tconst))       # Output: 1542357
length(unique(principals$tconst)) # Output: 1542357
length(unique(ratings$tconst))    # Output: 1542357
```

Now, all the datasets have the same set of unique `tconst` values.

However, by the nature of the `principals` dataset, we can't store its multiple 
rows for each `tconst` value into one row in our final data table.  

```{r diagnostics-3}
# DIAGNOSTICS: [PART 3]
# [1] Check whether the datasets can be merged
length(basics$tconst)     # This is expected. 
length(crew$tconst)       # This is expected. 
length(principals$tconst) # This needs to be fixed.
length(ratings$tconst)    # This is expected. 

# [2] Check one row of principals
principals_first_tconst <- principals[1, tconst]
principals[principals$tconst == principals_first_tconst]
# For this film, it seems that there are four rows associated with it. 

# [3] Modify principals to include data for each tconst in one row
# Use ordering as order of each individual. Index value represents ordering. 
principals <- principals[, .(
  nconst = list(nconst),
  category = list(category),
  job = list(job),
  characters = list(characters)
  ), by = tconst]
head(principals$characters)

# [4] We see that the characters column has an issue with "[\" and "\]". 
# We get rid of those symbols. 
principals[, characters := lapply(characters, function(x) {
  gsub('^\\["|"]$', '', x)
})]
```

Now, we can safely merge all datasets. 

```{r diagnostics-4}
# DIAGNOSTICS: [PART 4]
full_dt <- basics[crew, on = "tconst"]
full_dt <- full_dt[principals, on = "tconst"]
full_dt <- full_dt[ratings, on = "tconst"]
nrow(full_dt) # Output: 1,542,357. This is good. 
# saveRDS(full_dt, "full_dt.rds") # Run this line to save this cleaned df
```

TODO: Delete this later
```{r}
library(data.table)
library(mgcv)
gc()
setwd("~/Documents/06.sp25/04.stsci4100/stsci4100")
full_dt <- readRDS("full_dt.rds")
```

---

\newpage

# [3] DATA ANALYSIS:
Now that we have gone through the datasets and obtained a final data table 
`full_dt` to work with, we'll tackle **five questions** starting with the most 
intuitive one first.

## QUESTION 1:
Our first question is: **does the number of votes affect the ratings?**

In the real world, many people want to know how good a movie is before watching 
it. To do so, they go online to check the movie's reviews. But here's the thing 
everyone suspects:

+ Films that have very few votes (i.e. unpopular films) may not have the most 
reliable ratings, as their reviews don't contain much information. 
+ Films that have lots of votes (i.e. popular films) may suggest that the movie 
has larger positive reception or audience appeal. 

Specifically, we'll investigate whether or not films that have more votes tend 
to have higher or lower ratings (i.e. whether popularity is correlated with 
perceived quality). 

This will provide a good opening introduction, and hopefully provide some 
intuition, to our analysis. 

```{r q1-1}
# [1] Summary of our data's distribution of numVotes
summary(full_dt$numVotes)

# [2] Visualizing the distribution of the numVotes column
par(mfrow = c(1, 2))
hist(full_dt$numVotes,
     xlab = "Number of votes",
     ylab = "Frequency",
     main = "Histogram of Number of Votes",
     col = "azure1")
boxplot(full_dt$numVotes,
        xlab = "Number of votes",
        main = "Boxplot of Number of Votes",
        col = "black",
        pars = list(outpch = 16, outcol = "red"))
```

Outliers are colored red, and we see that the box plot deems most of the
visible points as outliers. The information so far suggests a heavy right skew. 

As we can see, the number of votes in our dataset is extremely right-skewed, 
where:

+ Most movies have very few votes. 
+ Few movies have lots of votes. 
+ So far, the data doesn't seem to follow a strictly linear pattern. The data 
seems to follow some sort of nonlinear pattern, however. Let us fit a local 
regression model — a non-parametric statistical method — to visualize the 
trajectory when we logarithmize `numVotes` (x).

```{r, q1-2}
# [3] Plot numVotes (x) against averageRating (y)
par(mfrow = c(1, 1))
x <- full_dt$numVotes
y <- full_dt$averageRating
plot(x = x,
     y = y,
     xlab = "Number of votes",
     ylab = "Average rating",
     main = "Number of votes vs. Average rating",
     col = "deepskyblue3")

# [4] Fit local regression model
# Fitting a LOWESS model with default arguments
# (Computationally more efficient than LOESS)
my.lowess <- lowess(log10(x), y)
# LOWESS curve
lines(10^my.lowess$x, my.lowess$y, col = "deeppink3", lwd = 2)

# [5] Check the strength of the linear relationship between these variables.
cor(log10(x), y)
```

The local regression model doesn't seem to give us a helpful fitted line on the 
trend of `averageRating` for each film. Let us try plotting it again but also 
bin the logarithm of the x-axis and sample a number of values rather than 
relying on the multitude of values heavily focused on the left side of the 
graph. 

```{r, q1-4}
# [6] Bin numVotes and sample a number of averageRatings for each bin
q1_dt <- full_dt[, .(numVotes, averageRating)]

# Logarithmize
q1_dt$x_log <- log10(q1_dt$numVotes)
# Bin log10(numVotes)
bin_width <- 0.1
q1_dt[, bin := floor(x_log / bin_width) * bin_width]
# Sample 500 (max) per bin
q1_dt_sampled <- q1_dt[, {
  idx <- sample(.N, min(.N, 500))
  .SD[idx]
}, by = bin]
# IQR ribbon
q1_stats <- q1_dt[, .(
  p25 = quantile(averageRating, 0.25),
  p75 = quantile(averageRating, 0.75),
  mean_rating = mean(averageRating),
  x_bin = 10^mean(x_log)
), by = bin][order(x_bin)]
```

Let us fit a Generalized Additive Model (GAM) to capture the non-linearity 
between `numVotes` and `averageRating` — in our case, modeling how 
averageRating changes with the logarithmic number of votes `log10(numVotes)`.

We hope that this approach gives us a more stable and interpretable fit than 
LOWESS when dealing with our data.

```{r}
# [7] Fit GAM
gam1 <- gam(averageRating ~ s(x_log), data = q1_dt)
# Predict
log_x_pred <- seq(log10(min(q1_dt$numVotes)), log10(max(q1_dt$numVotes)), 
              length.out = 500)
x_pred <- 10^log_x_pred
y_pred <- predict(gam1, newdata = data.frame(x_log = log_x_pred))
```

Let us plot this first with the x-axis as the logarithmic number of votes. 

```{r, echo = FALSE}
# [8]
# Plot
plot(NA,
     xlim = range(q1_stats$x_bin),
     ylim = range(q1_dt_sampled$averageRating, na.rm = TRUE),
     log = "x",
     xlab = "log10(numVotes)",
     ylab = "averageRating",
     main = "Avg. Rating vs. Log10 of Number of Votes"
)
mtext("Data binned (log10 scale) and sampled", side = 3, line = 0.5, cex = 0.9)

# 25–75% IQR Shading
polygon(c(q1_stats$x_bin, rev(q1_stats$x_bin)),
        c(q1_stats$p25, rev(q1_stats$p75)),
        col = adjustcolor("lightblue", alpha.f = 0.4), border = NA)
# Mean Line
lines(q1_stats$x_bin, q1_stats$mean_rating, col = "deeppink3", lwd = 2)
# Sampled points
points(q1_dt_sampled$numVotes, q1_dt_sampled$averageRating, 
       pch = 16, col = rgb(0, 0, 0, 10, maxColorValue = 255), cex = 0.5)
# Plot GAM prediction line
lines(x_pred, y_pred, col = "forestgreen", lwd = 2, lty = 1)

# Legend
legend("bottomright",
       legend = c("Sampled ratings", "Mean per bin", "25–75% range", 
                  "GAM prediction"),
       col = c("black", "deeppink3", rgb(135, 206, 250, maxColorValue = 255), 
               "forestgreen"),
       pch = c(16, NA, 15, NA), lwd = c(NA, 2, NA, 2), 
       lty = c(NA, 1, NA, 1), pt.cex = 1.5)
```

This plot shows strong evidence that as the number of votes increases, a film's 
`averageRating` fluctuates until it has a steep increase for its rating if the 
film gets an extremely high number of votes. 

Let us now plot this with the x-axis as the original number of votes and see if 
the same trend appears. 

```{r, echo = FALSE}
# [9]
# Plot
plot(NA,
     xlim = range(q1_stats$x_bin),
     ylim = range(q1_dt_sampled$averageRating, na.rm = TRUE),
     xlab = "numVotes",
     ylab = "averageRating",
     main = "Avg. Rating vs. Number of Votes"
)
mtext("Data binned (log10 scale) and sampled", side = 3, line = 0.5, cex = 0.9)

# 25–75% IQR Shading
polygon(c(q1_stats$x_bin, rev(q1_stats$x_bin)),
        c(q1_stats$p25, rev(q1_stats$p75)),
        col = adjustcolor("lightblue", alpha.f = 0.4), border = NA)
# Mean Line
lines(q1_stats$x_bin, q1_stats$mean_rating, col = "deeppink3", lwd = 2)
# Sampled points
points(q1_dt_sampled$numVotes, q1_dt_sampled$averageRating, 
       pch = 16, col = rgb(0, 0, 0, 10, maxColorValue = 255), cex = 0.5)
# Plot GAM prediction line
lines(x_pred, y_pred, col = "forestgreen", lwd = 2, lty = 1)
# Legend
legend("bottomright",
       legend = c("Sampled ratings", "Mean per bin", "25–75% range", 
                  "GAM prediction"),
       col = c("black", "deeppink3", rgb(135, 206, 250, maxColorValue = 255), 
               "forestgreen"),
       pch = c(16, NA, 15, NA), lwd = c(NA, 2, NA, 2), 
       lty = c(NA, 1, NA, 1), pt.cex = 1.5)
```

Again, we see a similar trend where a majority of the films with less than 
500,000 votes fluctuate heavily in `averageRating`, but as the number of votes 
increases steeply, the average rating values also gradually increase. Note that 
this plot follows the same GAM as before on the logarithmic `numVotes` variable.

Looking, at the summary of our GAM: 
```{r, echo = FALSE}
summary(gam1)
```

The GAM holds a p-value less than 2e-16. Seeing our plots, we see a stable 
horizontal line for its prediction line until higher values of `numVotes`. 
Thus, we have good evidence from the extremely low p-value that this trend is 
true. Additionally, it seems that the average rating for a film with not too 
extremely large number of votes is about 6.95017. 

Lastly, here is a boxplot in support of our analysis. 
```{r q1-5}
q1_dt$numVote_category <- NA

# Category 1: Extremely Low
q1_dt$numVote_category[q1_dt$numVotes <= 250000] <- "Extremely Low (<250K)"
# Category 2: Low
q1_dt$numVote_category[q1_dt$numVotes > 250000 
                     & q1_dt$numVotes <= 500000] <- "Low (251K–500K)"
# Category 3: Medium
q1_dt$numVote_category[q1_dt$numVotes > 500000
                     & q1_dt$numVotes <= 750000] <- "Medium (501K–750K)"
# Category 4: High
q1_dt$numVote_category[q1_dt$numVotes > 750000
                     & q1_dt$numVotes <= 1000000] <- "High (751K-1M)"
# Category 5: Extremely High
q1_dt$numVote_category[q1_dt$numVotes > 1000000] <- "Extremely High (>1M)"

q1_dt$numVote_category <- factor(q1_dt$numVote_category, 
                               levels = c("Extremely Low (<250K)",
                                          "Low (251K–500K)",
                                          "Medium (501K–750K)",
                                          "High (751K-1M)",
                                          "Extremely High (>1M)"))

boxplot(averageRating ~ numVote_category,
        data = q1_dt,
        main = "Average Rating by Category",
        xlab = "Vote Count",
        ylab = "Average Rating",
        col = "azure1")
```

The apperance of this box plot follows the same idea that higher values of 
vote counts indicate higher average ratings, only if extremely high. 

Here's what we can take away from this analysis:

1. The plotted graph of average ratings against the number of votes exhibits 
some nonlinear pattern.
   + Specifically, films that have less votes tend to have a wider spread of 
   average ratings, roughly between 2 to 8.
   + Specifically, films that have more votes tend to have a smaller spread of 
   average ratings.
2. As the number of votes increases, the average ratings stays stable at around 
7-8 before increasing as a film gets a much larger number of votes. 
3. Movies with extremely high votes (at least a million) have narrow/tight 
distributions with a much higher average rating than the others. This suggests 
that the number of votes a film receives can act as an indicator of the film's 
resulting ratings, only if the number of votes (`numVotes`) is extremely high. 

\newpage

## QUESTION 2:
The next question we want to answer is: 
**does having a writer affect the ratings?**

Let's take a step back to understand why we're even asking this question. Upon 
a quick glance at the dataset, we can see that its column **writers** is filled 
with a mixture of filled and missing values. In other words, not every film has 
a specified writer.

As a result, it naturally follows to ask how this would affect the film's 
ratings. Here's our approach to investigating this:

```{r q2-1}
q2_dt <- full_dt

# [1] Define a binary column called haswriter.
#  Binary column: 1 if there's a writer, 0 otherwise.
q2_dt$haswriter <- as.numeric(!is.na(q2_dt$writers))

# [2] Obtain a summary statistic and mean for the column haswriter.
summary(q2_dt$haswriter)
mean(q2_dt$haswriter)
```

Here, the min and max are obviously 0 and 1. The mean of the column `haswriter` 
is 0.7758. This means that roughly 77.6% of our films in the dataset have a 
specified writer.

```{r q2-2}
# [3] Obtain a summary statistic and mean for the column averageRating.
summary(q2_dt$averageRating)
mean(q2_dt$averageRating)
```

The worst films have an average rating of 1/10. The best films have an average 
rating of 10/10. The **average** average rating, as mentioned before, is 
6.95/10.

```{r q2-3}
# [4] Plot a boxplot of these summary statistics.
boxplot(averageRating ~ haswriter, data = q2_dt,
        col = c("red", "blue"),
        names = c("No Writer", "Writer"),
        ylab = "Average Rating",
        main = "Films without vs. with Specified Writers")

# Here, there doesn't seem to be much of a difference.
# Let's conduct a t-test to see if the difference is statistically significant.

# [5] Conduct the t-test.
t.test(averageRating ~ haswriter, data = q2_dt)
```

Here's our takeaway from our analysis:

1. In our comparison of films that have specified and unspecified writers, we 
can see that their boxplots are identical. Their distribution's span are 
relatively the same with identical means around 7.0.
2. There visually doesn't seem to be much of a difference between their means.
3. Just to be safe, we conducted a two-sample t-test. This is where things got 
interesting:
   + The p-value is less than 2.2e-16, which is less than an assumed 
   significance level of alpha = 0.05.
   + Technically, this means we reject the null and conclude there is enough 
   evidence to suggest there is a statistically significant difference between 
   these two means.
   + But this obviously does not align with our findings from the dataset.
   + In addition, the t-test's results shows us that the mean for "No Writer" 
   is 7.024177 while the mean for "Writer" is 6.928782. Those means are 
   incredibly close and identical in the **practical** sense. We simply do not 
   care whether a film has a rating that is 0.095395/10 higher than another. 
4. To get around this caveat, we asked ourselves two questions:
   + How could a film not have writers? They do; it's just that some films in 
   the dataset have no specified writers because of data incompleteness, 
   especially for really old or lesser-known films where the writers may have 
   not been well documented.
   + Do all audiences necessarily care about who the writers are for an 
   upcoming film? Unless it's a well-known writer who is mentioned in the 
   trailers, probably not! They likely pay more attention to the director(s) 
   and actors.
   + This is an example where the context of the situation need to be 
   considered, not just a statistical test's dry facts.
   
All in all, having a writer or not in the given dataset does not seem to affect 
the ratings of a film.

However, can it be the case that the **number of writers affects the ratings?**\

We can test this by counting the number of writers and 

```{r}
q2_dt$num_writers <- sapply(strsplit(q2_dt$writers, ","), length)
# Replace NA with 0
q2_dt$num_writers[is.na(q2_dt$writers)] <- 0
# Check distribution
summary(q2_dt$num_writers)
# Box plot to visualize distribution
boxplot(q2_dt$num_writers, 
        ylab = "Number of Writers", 
        main = "Box plot of Number of Writers", 
        col = "lightblue")
```

```{r}
num_writers_thresholds <- c(1, 2, 3, 5, 10, 20, 50, 100)
num_writers_table <- sapply(num_writers_thresholds, 
                            function(thresh) { 
                              mean(q2_dt$num_writers >= thresh) 
                            })
names(num_writers_table) <- paste0("≥", num_writers_thresholds, " writers")
round(100 * num_writers_table, 2)
```

```{r}
hist(pmin(q2_dt$num_writers, 20), 
     breaks = 0:20,
     right = FALSE,
     col = "lightblue",
     main = "Histogram of Writer Counts (capped at 20)",
     xlab = "Number of Writers")
```


```{r}
q2_dt <- q2_dt[q2_dt$num_writers <= 20, ]
boxplot(averageRating ~ num_writers, data = q2_dt,
        col = "lightblue",
        xlab = "Number of Writers",
        ylab = "Average Rating",
        main = "Boxplot: Average Rating by Number of Writers")
```


```{r}
# Fit the linear model
lm_simple <- lm(averageRating ~ num_writers, data = q2_dt)

# View summary
summary(lm_simple)
```

```{r}

```











```{r}
# Prepare data
q2_dt$x_log <- log10(q2_dt$numVotes)

# Fit GAM model
library(mgcv)
gam_multi <- gam(averageRating ~ s(x_log) + s(startYear) + s(num_writers), data = q2_dt)

# Summary
summary(gam_multi)
```


```{r}
par(mfrow = c(1, 3))
plot(gam_multi, select = 1, main = "Effect of log10(Votes)")
plot(gam_multi, select = 2, main = "Effect of Start Year")
plot(gam_multi, select = 3, main = "Effect of Num Writers")
```



```{r}
library(data.table)

# Step 1: Flatten `nconst` and `category` columns from full_dt
long_principals <- full_dt[, .(tconst, nconst, category)]

# Unnest the parallel lists (each index corresponds to a person/role)
long_principals <- long_principals[
  , .(nconst = unlist(nconst), 
      category = unlist(category)), 
  by = tconst
]

# Step 2: Count total film appearances per nconst per category
role_experience <- long_principals[, .N, by = .(nconst, category)]
setnames(role_experience, "N", "experience")

# Step 3: Define reusable function to get per-film experience stats
get_role_experience_by_film <- function(role_name) {
  # Subset experience by role
  role_exp <- role_experience[category == role_name]
  
  # Extract relevant rows from full_dt
  role_map <- full_dt[, .(tconst, nconst, category)]
  role_map <- role_map[, .(
    nconst = unlist(nconst),
    category = unlist(category)
  ), by = tconst]
  role_map <- role_map[category == role_name]
  
  # Merge in experience
  role_map <- merge(role_map, role_exp, by = "nconst", all.x = TRUE)
  
  # Aggregate stats per film
  role_exp_by_film <- role_map[, .(
    exp_mean = mean(experience, na.rm = TRUE),
    exp_max = max(experience, na.rm = TRUE)
  ), by = tconst]
  
  # Rename columns
  setnames(role_exp_by_film, 
           c("exp_mean", "exp_max"), 
           c(paste0(role_name, "_exp_mean"), paste0(role_name, "_exp_max")))
  
  return(role_exp_by_film)
}

# Step 4: Compute experience stats for writer, director, actor
writer_exp_by_film <- get_role_experience_by_film("writer")
director_exp_by_film <- get_role_experience_by_film("director")
actor_exp_by_film <- get_role_experience_by_film("actor")

# Step 5: Merge into q2_dt
q2_dt <- merge(q2_dt, writer_exp_by_film, by = "tconst", all.x = TRUE)
q2_dt <- merge(q2_dt, director_exp_by_film, by = "tconst", all.x = TRUE)
q2_dt <- merge(q2_dt, actor_exp_by_film, by = "tconst", all.x = TRUE)

# Step 6: Fill NAs with 0 (films with no writer/director/actor info)
exp_cols <- c("writer_exp_mean", "writer_exp_max",
              "director_exp_mean", "director_exp_max",
              "actor_exp_mean", "actor_exp_max")

for (col in exp_cols) {
  q2_dt[is.na(get(col)), (col) := 0]
}
```

```{r}
# GAM
library(mgcv)
# Fit model
gam_exp <- gam(averageRating ~ 
                 s(writer_exp_mean) +
                 s(director_exp_mean) +
                 s(actor_exp_mean),
               data = q2_dt)
# Summary
summary(gam_exp)
# Plot
par(mfrow = c(1, 3))
plot(gam_exp, select = 1, main = "Writer Experience")
plot(gam_exp, select = 2, main = "Director Experience")
plot(gam_exp, select = 3, main = "Actor Experience")
```








\newpage

